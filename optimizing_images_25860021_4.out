vitb16
say
vit_base
Namespace(arch='vit_base', patch_size=16, pretrained_weights='/scratch/eo41/dino/models_vitb16/say_5fps_vitb16_checkpoint.pth', checkpoint_key='teacher', batch_size=8192, dist_url='env://', local_rank=0, top_k_imgs=100, val_data_path='/vast/eo41/data/ecoset/train', num_workers=4, output_dir='/scratch/eo41/dino/optimizing_images/ecoset', save_prefix='say_vitb16')
Launced with slurm
world size, rank, gpu, device count: 1 0 0 1
| distributed init (rank 0): env://
git:
  sha: 15bf10fb4633c947218e4c5f144a1908484ab3c2, status: has uncommited changes, branch: master

arch: vit_base
batch_size: 8192
checkpoint_key: teacher
dist_url: env://
gpu: 0
local_rank: 0
num_workers: 4
output_dir: /scratch/eo41/dino/optimizing_images/ecoset
patch_size: 16
pretrained_weights: /scratch/eo41/dino/models_vitb16/say_5fps_vitb16_checkpoint.pth
rank: 0
save_prefix: say_vitb16
top_k_imgs: 100
val_data_path: /vast/eo41/data/ecoset/train
world_size: 1
Take key teacher in provided checkpoint dict
Pretrained weights found at /scratch/eo41/dino/models_vitb16/say_5fps_vitb16_checkpoint.pth and loaded with msg: _IncompatibleKeys(missing_keys=[], unexpected_keys=['head.mlp.0.weight', 'head.mlp.0.bias', 'head.mlp.2.weight', 'head.mlp.2.bias', 'head.mlp.4.weight', 'head.mlp.4.bias', 'head.last_layer.weight_g', 'head.last_layer.weight_v'])
Model vit_base built.
/scratch/eo41/miniconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py:287: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Data loaded: dataset contains 1444911 images, and takes 177 training iterations per epoch.
Traceback (most recent call last):
  File "/scratch/eo41/dino/optimizing_images.py", line 181, in <module>
    find_optimizing_imgs(args)
  File "/scratch/eo41/dino/optimizing_images.py", line 89, in find_optimizing_imgs
    preds, labels = forward_imgs(val_loader, model, args)
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/eo41/dino/optimizing_images.py", line 138, in forward_imgs
    intermediate_output = model.get_intermediate_layers(inp)
  File "/scratch/eo41/dino/vision_transformer.py", line 228, in get_intermediate_layers
    x = blk(x)
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/eo41/dino/vision_transformer.py", line 110, in forward
    x = x + self.drop_path(self.mlp(self.norm2(x)))
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/eo41/dino/vision_transformer.py", line 61, in forward
    x = self.act(x)
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 652, in forward
    return F.gelu(input)
  File "/scratch/eo41/miniconda3/lib/python3.9/site-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 18.47 GiB (GPU 0; 79.17 GiB total capacity; 56.06 GiB already allocated; 16.71 GiB free; 60.37 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
srun: error: ga001: task 0: Exited with exit code 1
srun: launch/slurm: _step_signal: Terminating StepId=25860021.0
Done
